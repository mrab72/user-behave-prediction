{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InternetAccess.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrab72/user-behave-prediction/blob/master/Internet-access-text-gen-lang-model-LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "772Hlsv6Gyep"
      },
      "source": [
        "## Connect to the google drive:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWk48VPWrW_u",
        "outputId": "f714a30e-e9e2-486d-c5b7-010d26291672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyBZ-2eFsTqq"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ssYcybrqsr"
      },
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "import operator\n",
        "from django.utils.dateparse import parse_datetime\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdRBLTd2scGE"
      },
      "source": [
        "## Load first user data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwDttiGisRuN"
      },
      "source": [
        "with open ('/content/drive/My Drive/tubp/file_84_241_9_79_apr.json', 'r') as f:\n",
        "  data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOIseJyJtKn7"
      },
      "source": [
        "## Split Data into the 5_minutes chunks\n",
        "by majority voting choose, label behavior of each time steps.\n",
        "here each step is 5 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_vJAsWtCMP6"
      },
      "source": [
        "### parsed date from data:**bold text**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTfwhCru434"
      },
      "source": [
        "parsed_data = {}\n",
        "for date, traffic in data.items():\n",
        "  parsed_date = datetime.strptime(date.split('.')[0], '%Y-%m-%d-%H:%M:%S')\n",
        "  parsed_data[parsed_date] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqJX0D0Av7qa"
      },
      "source": [
        "ordered_data = OrderedDict(sorted(parsed_data.items(), key=lambda t: t[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULh4mb0CR4S"
      },
      "source": [
        "### create 5 minutes sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yuBIIJ59Nwn"
      },
      "source": [
        "internet_access_seq = OrderedDict(([(datetime(2020, 1, 6, 12, 55), 0)]))\n",
        "date = datetime(2020, 1, 6, 12, 55)\n",
        "\n",
        "while date < datetime(2020,3,25,9,36):\n",
        "  date = date + timedelta(0, 60*15)\n",
        "  internet_access_seq[date] = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsW9V8j3zX5d"
      },
      "source": [
        "for date, value in ordered_data.items():\n",
        "  new_date = (date - timedelta(0, 60 * (date.minute  % 15))).replace(second=0)\n",
        "  internet_access_seq[new_date] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiJCzAl7GvXG"
      },
      "source": [
        "## Predict Internet Access by Text generation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRL6zk3HEAJ"
      },
      "source": [
        "traffics = list(internet_access_seq.values())\n",
        "dates = list(internet_access_seq.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJO2Mkc7ApUd",
        "outputId": "8ed0a5cf-ef25-4563-c249-b6a89235887a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(traffics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14469"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0j_hZTHubf"
      },
      "source": [
        "### prepare requrired methods for rnn model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNMoy1gaYB4t"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqO_ExBTH2L1"
      },
      "source": [
        "def _prepare_dataset(data):\n",
        "  # dictionary = list(set(data))\n",
        "\n",
        "  # char2idx = {u:i for i, u in enumerate(dictionary)}\n",
        "  # idx2char = np.array(dictionary)\n",
        "\n",
        "  # text_as_int = np.array([char2idx[c] for c in data])\n",
        "\n",
        "  # The maximum length sentence we want for a single input in characters\n",
        "  seq_length = 100\n",
        "  # examples_per_epoch = len(data) // (seq_length+1)\n",
        "\n",
        "  # Create training examples / targets \n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  return dataset\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M1YXCK2NJ-v"
      },
      "source": [
        "### build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFeirFTXICE3"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        vocab_size,\n",
        "          embedding_dim,\n",
        "          batch_input_shape=[batch_size, None]\n",
        "          ),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtepfi-IIGdM"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      labels, logits, from_logits=True\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF8ACF3uIKgG"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K77ve2pHYVVb",
        "outputId": "f15ab0e6-b5a3-446a-d941-f93ec4f584c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 8\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "dataset = _prepare_dataset(traffics[:int(len(traffics)/2)])\n",
        "time_model = build_model(\n",
        "  vocab_size = 2,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE\n",
        "  )\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS=10\n",
        "\n",
        "time_model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = time_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "71/71 [==============================] - 65s 918ms/step - loss: 0.0197\n",
            "Epoch 2/10\n",
            "71/71 [==============================] - 65s 919ms/step - loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "71/71 [==============================] - 65s 921ms/step - loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "71/71 [==============================] - 65s 912ms/step - loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "71/71 [==============================] - 65s 915ms/step - loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "71/71 [==============================] - 65s 916ms/step - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "71/71 [==============================] - 69s 974ms/step - loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "71/71 [==============================] - 65s 915ms/step - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "71/71 [==============================] - 65s 916ms/step - loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "71/71 [==============================] - 65s 916ms/step - loss: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR7ZZhGn6qyM"
      },
      "source": [
        "### Generate Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBGijH5K5qu7"
      },
      "source": [
        "def generate_sequence(model, start_string, num_generate):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = num_generate\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = start_string\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(predicted_id)\n",
        "\n",
        "  return text_generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlDoZSlm70NF"
      },
      "source": [
        "### evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDGB3ao17FL9"
      },
      "source": [
        "next_internet_access = generate_sequence(\n",
        "    time_model, start_string=traffics[:int(len(traffics)/2)], num_generate=100*5\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jm4NnQT7lkR"
      },
      "source": [
        "true_traffic_access  = traffics[int(len(traffics)/2):int(len(traffics)/2) + 100*5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmE08mVg8yiC"
      },
      "source": [
        "#### Accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOqzzHi28IXu",
        "outputId": "8f84b513-405e-49cf-e202-9384bb5d33ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(np.array(true_traffic_access) == np.array(next_internet_access)) / 500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y66zzty82J9"
      },
      "source": [
        "#### recall and precision:\n",
        "![recall and precision](https://en.wikipedia.org/wiki/File:Precisionrecall.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkK-quHC84G1"
      },
      "source": [
        "true_traffic_access = np.array(true_traffic_access)\n",
        "next_internet_access = np.array(next_internet_access)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esJsYiIhAOOh"
      },
      "source": [
        "##### True Positive(1)(tp):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyUhd2c-_IxK"
      },
      "source": [
        "predicted_p_index = np.where(next_internet_access == 1)[0]\n",
        "predicted_n_index = np.where(next_internet_access == 0)[0]\n",
        "\n",
        "true_p_index = np.where(true_traffic_access == 1)[0]\n",
        "true_n_index = np.where(true_traffic_access == 0)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu-VSoik_aZh"
      },
      "source": [
        "tp = 0\n",
        "for item in predicted_p_index:\n",
        "  if item in true_p_index:\n",
        "    tp += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHHsFnF8ATjk"
      },
      "source": [
        "##### False Positive(1)(fp):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E4KBO04Aeww"
      },
      "source": [
        "tp = 0\n",
        "for item in predicted_p_index:\n",
        "  if item in true_p_index:\n",
        "    tp += 1\n",
        "fp = 0\n",
        "for item in predicted_p_index:\n",
        "  if item not in true_p_index:\n",
        "    fp += 1\n",
        "fn = 0\n",
        "for item in predicted_n_index:\n",
        "  if item in true_n_index:\n",
        "    fn += 1\n",
        "# precision = tp / (tp + fp)\n",
        "# recall = tp / (tp + fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v78amePkA7gD"
      },
      "source": [
        "##### False Negative(0)(fn):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ZfoVxrA_-M"
      },
      "source": [
        "predicted_n_index = np.where(true_traffic_access == 0)[0]\n",
        "true_n_index = np.where(next_internet_access == 0)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe5lD0qaBM0s"
      },
      "source": [
        "fn = 0\n",
        "for item in predicted_n_index:\n",
        "  if item in true_n_index:\n",
        "    fn += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFU1tejAv_C"
      },
      "source": [
        "##### precision:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMi0NkrAAycj",
        "outputId": "47d442c7-1078-4a20-837e-5dab80e16bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "tp / (tp + fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5f4c7eff9bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icwZYdN6A2VE"
      },
      "source": [
        "##### recall:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUoBCC3LA1_6",
        "outputId": "aa95b648-7b68-4632-9dbc-2d38afcb0ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tp / (tp + fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmmhHNXyPgfc"
      },
      "source": [
        "#### Save Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnJAZJ2KPikN",
        "outputId": "95765631-26a5-4530-f6da-f265cec89242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "time_model.save(\n",
        "    \"interent_access_model\", overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: interent_access_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nzHQUeJT0Zu"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/interent_access_model\", custom_objects={'loss': loss})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}